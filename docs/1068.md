# 什么是 CoreML？

> 原文:[https://www.javatpoint.com/what-is-coreml](https://www.javatpoint.com/what-is-coreml)

在当今的科技世界中，机器学习是一项需要花费大量时间的工作，iOS 应用程序也是如此。CoreML 允许我们将机器学习集成到 iOS 应用程序中。它执行两个主要任务。

1.  它方便我们加载预先训练好的机器学习模型。CoreML 帮助我们将预训练的 MLModels 转换成可以与 XCode 一起使用的类。
2.  CoreML 允许我们进行预测。用户下载应用程序后，我们会将模型本地加载到设备上。我们的应用程序将使用该模型对图像识别、语音识别等进行预测。

我们可以去[https://developer.apple.com/machine-learning/](https://developer.apple.com/machine-learning/)了解如何使用机器学习将其与 iOS 应用程序相结合。它还在模型部分提供了即用(预训练)模型，即[https://developer.apple.com/machine-learning/models/](https://developer.apple.com/machine-learning/models/)。

![What is CoreML](../Images/a25a8ee1a84d31edc484dde4125619db.png)

在本教程中，我们将使用 MobileNetV2 模型，该模型正在被训练来对相机帧或图像中的主要对象进行分类。我们将使用它来识别用户在应用程序中拍摄的照片。

然而，CoreML 不能使用应用程序生成的任何数据来进一步训练模型。应用程序中加载的预处理模型仍然是静态模型。我们还需要了解的是，应用程序中使用的 CoreML 没有加密；也就是说，如果我们需要对银行应用程序使用任何模型，比如股票市场，我们需要意识到我们不应该将它与 CoreML 一起使用，以将其与 [iOS](https://www.javatpoint.com/ios-development-using-swift) 应用程序合并。

CoreML 最大的优点是，即使我们对机器学习知之甚少，我们仍然可以利用它将我们的应用程序与机器学习结合起来。

### 创建 CoreML 应用程序

让我们创建一个 iOS 应用程序，它将使用 CoreML 来整合机器学习。正如我们在本教程中已经说过的，我们将使用[https://developer.apple.com/machine-learning/models/](https://developer.apple.com/machine-learning/models/)上提供的 MobileNetV2 预训练模型。这将检测相机帧中的主要对象。

要创建新的 XCode 项目，请转到 XCode 中的文件->新建->项目，然后选择单视图应用程序。现在，提供如下所示的捆绑包 id、应用名称、组织名称等应用信息。

![What is CoreML](../Images/8f002fca4cd126f16d92e261de87c662.png)

一旦我们完成了 iOS 应用的创建，我们需要做的第一件事就是在我们新的 XCode 项目中加入一个预先训练好的图像识别模型。为此，导航至[https://developer.apple.com/machine-learning/models/](https://developer.apple.com/machine-learning/models/)并选择移动网络 2 下载。mlmodel 文件。点击下载按钮下载 mlmodel 文件，如下所示。

![What is CoreML](../Images/cc4dd2c41b22949b1b8760d97daf6000.png)

一旦我们完成下载。mlmodel 文件，我们需要将其添加到 [XCode](https://www.javatpoint.com/ios-introduction-to-xcode-ide) 中。为此，请打开。mlmodel 文件被下载，并将其拖放到 XCode 中。

一旦我们这样做了，我们就会得到。mlmodel 文件添加到 XCode，如下所示。

![What is CoreML](../Images/70ac55d80dd66b04504a4bdfa00e36ca.png)

一旦我们打开这个文件，我们将在 XCode 中看到它的信息以及 XCode 创建的模型类。

![What is CoreML](../Images/ea63b2b101e3882a713bae468b4ce527.png)

让我们首先设置界面构建器。在故事板中，我们需要在导航控制器中嵌入我们的视图控制器，并添加一个条形按钮项目，以便在点击相机时打开它。我们还将向视图控制器添加一个图像视图，以显示从相机拍摄的图像。

![What is CoreML](../Images/01200b764af111f94899437557164e18.png)

现在，我们需要将我们的模型合并到视图控制器中。为此，我们需要在视图控制器类中导入 CoreML 和 Vision。我们还将使我们的视图控制器符合 UIImagePickerControllerDelegate 和 UINavigationControllerDelegate。

添加以下代码来实现应用程序的摄像头功能，当用户点击摄像头栏按钮项目时，摄像头将被打开。

```

import UIKit
import CoreML
import Vision

class ViewController: UIViewController, UIImagePickerControllerDelegate, UINavigationControllerDelegate {

    @IBOutlet weak var imgView: UIImageView!
    let imagePicker = UIImagePickerController()

    override func viewDidLoad() {
        super.viewDidLoad()
        // Do any additional setup after loading the view.
        imagePicker.delegate = self
        imagePicker.sourceType = .camera
        imagePicker.allowsEditing = false
    }

    @IBAction func tappedCamera(_ sender: UIBarButtonItem) {
        self.present(imagePicker, animated: true, completion: nil)
    }
}
We also need to add the following code to the info.plist file.

<key>NSCameraUsageDescription</key>
<string>App will use camera to take pictures </string>

```

现在，我们需要将捕获的图像发送到机器学习模型。为此，首先，我们需要在视图控制器中访问捕获的图像。添加委托方法，即 imagePickerController(_ picker:UIImagePickerController，DidFinishPickingMediaWithinfo:[UIImagePickerController。InfoKey : Any])到视图控制器。

```

func imagePickerController(_ picker: UIImagePickerController, didFinishPickingMediaWithInfo info: [UIImagePickerController.InfoKey : Any]) {
       if let img = info[UIImagePickerController.InfoKey.originalImage] as?
       UIImage {
             self.imgView.image = img
             self.dismiss(animated: true, completion: nil)
          }
          else {
             print("error")
          }
       }

```

现在，我们需要实现检测功能来检测我们捕获的图像。首先，我们需要将我们的图像转换为 CIImage，以便与我们的 MLModel 一起使用。

要将图像转换为 CIImage，请在调用 dismiss()后添加以下代码。

```

guard let ciImage = CIImage(image: img) else{
            debugPrint("Can't convert image to CIImage")
            return
        }

```

我们需要通过使用模型来检测这个 ciImage。为此，向视图控制器添加以下代码。

```

func detect(image:CIImage){
        guard let model = try? VNCoreMLModel(for: MobileNetV2().model) else{
            debugPrint("Can't convert model to VNCoreMLModel")
            return
        }
        let request = VNCoreMLRequest(model: model) { (request, error) in
            guard let result = request.results as? [VNClassificationObservation] else{
                debugPrint("")
                return
            }
            debugPrint(result)
        }
            let handler = VNImageRequestHandler(ciImage:image)
            do{
                try handler.perform([request])
            }catch{
                debugPrint(error)
            }

    }

```

我们还需要在获取 CIImage 后立即调用 detect()方法。我们还需要将 CIImage 传递给这个 detect()方法。添加以下代码来调用委托方法中的 detect()方法，在该方法中，我们获得了 CIImage 的对象。

```

detect(image: ciImage)

```

现在，构建并运行项目，捕捉任何图像，并查看我们正在获得的结果。让我们通过在网上浏览来点击计算机图像。

一旦我们拍下照片并继续，我们将看到结果打印在控制台上，如下所示。

![What is CoreML](../Images/a2eeb4cfe87d316c8e2f09e508879b5e.png)

正如我们所看到的，模型已经对图像进行了分类，并产生了包含图像可能包含的所有可能对象的结果。

现在，我们可以检查结果的第一个对象，因为它将是图像中最有可能的对象。在这个项目中，我们捕获一个汽车对象，并查看结果是否包含汽车作为第一个对象。

为此，在获取结果对象后，在完成处理程序中添加以下代码。

```

if let firstResult = result.first{
                if(firstResult.identifier.lowercased().contains("car")){
                    self.navigationItem.title = "Car"
                }else{
                    self.navigationItem.title = "Not Car"
                }
            }

```

这将根据捕获的对象更改导航栏标题。

我们的 ViewController.swift 包含以下代码。

```

import UIKit
import CoreML
import Vision

class ViewController: UIViewController, UIImagePickerControllerDelegate, UINavigationControllerDelegate {

    @IBOutlet weak var imgView: UIImageView!
    let imagePicker = UIImagePickerController()

    override func viewDidLoad() {
        super.viewDidLoad()
        // Do any additional setup after loading the view.
        imagePicker.delegate = self
        imagePicker.sourceType = .camera
        imagePicker.allowsEditing = false
    }

    @IBAction func tappedCamera(_ sender: UIBarButtonItem) {
        self.present(imagePicker, animated: true, completion: nil)
    }

    func imagePickerController(_ picker: UIImagePickerController, didFinishPickingMediaWithInfo info: [UIImagePickerController.InfoKey : Any]) {
       if let img = info[UIImagePickerController.InfoKey.originalImage] as?
       UIImage {
             self.imgView.image = img
             self.dismiss(animated: true, completion: nil)
        guard let ciImage = CIImage(image: img) else{
            debugPrint("Can't convert image to CIImage")
            return
        }
        detect(image: ciImage)
          }
          else {
             print("error")
          }
       }
    func detect(image:CIImage){
        guard let model = try? VNCoreMLModel(for: MobileNetV2().model) else{
            debugPrint("Can't convert model to VNCoreMLModel")
            return
        }
        let request = VNCoreMLRequest(model: model) { (request, error) in
            guard let result = request.results as? [VNClassificationObservation] else{
                debugPrint("")
                return
            }
            debugPrint(result)
            if let firstResult = result.first{
                if(firstResult.identifier.lowercased().contains("car")){
                    self.navigationItem.title = "Car"
                }else{
                    self.navigationItem.title = "Not Car"
                }
            }
        }
            let handler = VNImageRequestHandler(ciImage:image)
            do{
                try handler.perform([request])
            }catch{
                debugPrint(error)
            }

    }
}

```

* * *